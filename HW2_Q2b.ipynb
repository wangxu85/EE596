{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "from load_cifar import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Hyper-perparmeter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = .01\n",
    "#number of traning steps\n",
    "num_steps =50 # longer fitting model cycles \n",
    "#number of batch_size\n",
    "batch_size = 64 # 128*5 - more training data \n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 100 # 100 - reduce over fitting \n",
    "n_hidden_2 = 100\n",
    "n_hidden_3 = 100\n",
    "num_input = 32*32*3 # num of dimension of an image \n",
    "num_classes = 10 # num of label names \n",
    "\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.float32,[None,num_classes],name='Y')\n",
    "training = tf.placeholder_with_default(False, shape=(),name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Neural Network Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give l2 regularizer to weights \n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.1)\n",
    "# give batch normalization: by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.layers.dense(x,n_hidden_1,activation=None,kernel_initializer=tf.random_normal_initializer(), kernel_regularizer=regularizer)\n",
    "    layer_1_act = tf.nn.relu(layer_1_out)        \n",
    "    layer_1_bn = tf.layers.batch_normalization(layer_1_act,momentum=0.99,training=training)\n",
    "    \n",
    "    layer_2_drop = tf.layers.dropout(layer_1_bn ,dropout_rate,training=training)\n",
    "    layer_2_out = tf.layers.dense( layer_2_drop, n_hidden_2,activation=None,kernel_initializer=tf.random_normal_initializer(),kernel_regularizer=regularizer)\n",
    "    layer_2_act = tf.nn.relu(layer_2_out)        \n",
    "    layer_2_drop = tf.layers.dropout(layer_2_act ,dropout_rate,training=training)\n",
    "    layer_2_bn = tf.layers.batch_normalization(layer_2_act,momentum=0.99,training=training)\n",
    "    \n",
    "    layer_3_drop = tf.layers.dropout(layer_2_bn ,dropout_rate,training=training)\n",
    "    layer_3_out = tf.layers.dense( layer_3_drop, n_hidden_3,activation=None,kernel_initializer=tf.random_normal_initializer(),kernel_regularizer=regularizer)\n",
    "    layer_3_act = tf.nn.relu(layer_3_out)        \n",
    "    layer_3_drop = tf.layers.dropout(layer_3_act ,dropout_rate,training=training)\n",
    "    layer_3_bn = tf.layers.batch_normalization(layer_3_act,momentum=0.99,training=training)\n",
    "    \n",
    "    out = tf.layers.dense(layer_3_bn,num_classes,activation=None,kernel_initializer=tf.random_normal_initializer(),kernel_regularizer=regularizer)\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define cost and optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y),name='loss')\n",
    "l2_loss = tf.losses.get_regularization_loss()\n",
    "loss+=l2_loss\n",
    "\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))  \n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and testing</h1>\n",
    "<h2>1.Print out validation accuracy after each training poch</h2>\n",
    "<h2>2.Print out training time you spend on each epoch</h2>\n",
    "<h2>3.Print out testing accuracy in the end</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, validation accuracy= 0.092 ..time used: 2.5051\n",
      "step 10, validation accuracy= 0.138 ..time used: 1.5841\n",
      "step 20, validation accuracy= 0.182 ..time used: 1.7525\n",
      "step 30, validation accuracy= 0.146 ..time used: 1.5621\n",
      "step 40, validation accuracy= 0.099 ..time used: 1.6831\n",
      "Training finished!\n",
      "Testing accuracy: 0.078125\n"
     ]
    }
   ],
   "source": [
    "valid_x , valid_y = load_preprocessed_validation_batch()\n",
    "test_x , test_y = load_preprocessed_test_batch(test_mini_batch_size=batch_size)\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        start = time.time()\n",
    "        for batch_id in range(1,6):        \n",
    "            batch_x, batch_y = load_preprocessed_training_batch( batch_id=batch_id, mini_batch_size = batch_size)\n",
    "            #run optimization            \n",
    "            sess.run([train_op,extra_update_ops], feed_dict={X:batch_x, Y:batch_y,training:True})        \n",
    "            \n",
    "        end = time.time()\n",
    "        acc = sess.run(accuracy,feed_dict={X:valid_x, Y:valid_y})\n",
    "        if i%10==0:\n",
    "            print(\"step \"+str(i)+\", validation accuracy= {:.3f}\".format(acc) , '..time used:', round( end -start, 4) )\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "    print(\"Testing accuracy:\", sess.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and comment\n",
    "\n",
    "Unfortunately, after 50 epochs, I am not able to get a good result of accuracy on the validation dataset. Basically, my full connected neural network is not learning much. I am not sure why this happened. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
