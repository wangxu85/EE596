{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CharRNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ucqjU40At7J2","colab_type":"code","colab":{}},"source":["# load package \n","import time\n","import numpy as np\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_3DGrMpt7KD","colab_type":"code","colab":{}},"source":["# read in text for training \n","with open('sample_data/shakespeare.txt', 'r') as f:\n","    text = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlYBtOeOt7KO","colab_type":"code","colab":{}},"source":["# construct char set  \n","vocab = set(text)\n","vocab_to_int = {c: i for i, c in enumerate( vocab)}\n","int_to_vocab = dict( enumerate( vocab) )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBtD6Nc4t7Kc","colab_type":"code","colab":{}},"source":["# encode text \n","encoded = np.array(  [ vocab_to_int[c] for c in text ], dtype =np.int32 )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKqhsLsSt7Kj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"9938e90d-2c59-4a1a-fe28-1115e87cea25","executionInfo":{"status":"ok","timestamp":1559003795703,"user_tz":420,"elapsed":1590,"user":{"displayName":"Steven Wang","photoUrl":"https://lh4.googleusercontent.com/-RC2OL2L-CUs/AAAAAAAAAAI/AAAAAAAAD8I/C-L5xQPIj30/s64/photo.jpg","userId":"14479905213551862384"}}},"source":["# test \n","print( encoded[:10] )\n","print( text[:10] )\n","print( vocab_to_int['F'] )"],"execution_count":92,"outputs":[{"output_type":"stream","text":["[47 54 63 35 24 48  0 54 24 54]\n","First Citi\n","47\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c0k6PEEot7Kx","colab_type":"code","colab":{}},"source":["# get bactch\n","def get_batches( arr, num_seqs, num_steps):\n","    batch_size = num_seqs * num_steps \n","    n_batch = int(  len(arr) / batch_size )\n","    arr = arr[ : (batch_size * n_batch )] # keep only full batch size part \n","    # reshape \n","    arr = arr.reshape( (num_seqs, -1)  )\n","    \n","    for n in range(0, arr.shape[1] , num_steps):\n","        x  = arr[ :, n : ( n+num_steps )]\n","        y  = np.zeros_like(x)\n","        y[: , :-1] , y[:, -1] = x[: , 1: ], y[: , 0]\n","        # generating x, y only once, no memory storage using generator yield \n","        yield x ,y \n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2Zrx1nZt7K4","colab_type":"code","colab":{}},"source":["#tmp = get_batches(encoded , num_seqs = 10, num_steps = 7)\n","#counter = 0\n","#for x, y in tmp:\n","#    print(\"x\\n\", x.shape, '\\n' , x)\n","#    print(\"\\ny\\n\", y.shape, '\\n',  x)\n","#    counter = counter + 1 \n","#    if counter > 3:\n","#       break "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NnTorJFQt7K_","colab_type":"code","colab":{}},"source":["# create  placeholders \n","def build_inputs( num_seqs, num_steps ):\n","    # placeholder for input: size num_seqs\n","    inputs = tf.placeholder(tf.int32, shape= ( num_seqs, num_steps), name = 'inputs')\n","    targets = tf.placeholder(tf.int32, shape= ( num_seqs, num_steps), name = 'targets')\n","    \n","    # keep probability for dropout model use \n","    keep_prob = tf.placeholder( tf.float32, name = 'keep_prob')\n","    \n","    return inputs, targets, keep_prob\n","\n","#i, t, k = build_inputs( 3, 5)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOw8AxaLt7LG","colab_type":"code","colab":{}},"source":["# build LSTM \n","def make_cell(lstm_size):\n","    return tf.nn.rnn_cell.BasicLSTMCell(lstm_size, state_is_tuple=True)\n","\n","def build_lstm( lstm_size, num_layers, num_seqs, keep_prob):\n","    # lstm_size: num hidden units \n","    # num  layers: multiple layers/cells of LSTM \n","    lstm = tf.nn.rnn_cell.BasicLSTMCell( lstm_size)\n","    \n","    # dropout to avoid overfitting \n","    lstm_drop =tf.nn.rnn_cell.DropoutWrapper( lstm, output_keep_prob= keep_prob) \n","    \n","    # multi-layer lstm : stack multiple lstm_drop cells as one lstm cell\n","    cell = tf.nn.rnn_cell.MultiRNNCell([make_cell(lstm_size) for _ in range(num_layers)], \n","                                state_is_tuple=True)\n","    #cell = tf.nn.rnn_cell.MultiRNNCell( [ lstm_drop for _ in range(num_layers)]  )\n","    initial_state = cell.zero_state(num_seqs, tf.float32)\n","    \n","    return cell, initial_state "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qeojto4wt7LX","colab_type":"code","colab":{}},"source":["# build output \n","def build_output( lstm_output, in_size , out_size ):\n","    # in_size: size of lstm output \n","    # out_size: num of classes to predict e.g. len( vocab )\n","    seq_output = tf.concat( lstm_output  , axis=1)\n","    x = tf.reshape( seq_output , [-1, in_size])\n","    \n","    # create Variable for softmax \n","    with tf.variable_scope('softmax'):\n","        softmax_w = tf.Variable( tf.truncated_normal( [in_size, out_size], stddev=0.1 )  )\n","        softmax_b = tf.Variable( tf.zeros( out_size ))\n","        \n","    logits = tf.matmul(  x, softmax_w) + softmax_b \n","    \n","    out = tf.nn.softmax( logits, name = 'prediction' )\n","    \n","    return  out, logits\n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGgfzhNVt7Le","colab_type":"code","colab":{}},"source":["# loss function \n","def build_loss( logits, targets, lstm_size, num_classes  ):\n","    # one hot encoding to a binary vector of size num_classes, where position of 1 indicate value of targets \n","    # num_classes: vocab_size \n","    \n","    y_one_hot = tf.one_hot( targets , num_classes )\n","    y_reshaped = tf.reshape( y_one_hot, logits.get_shape() )\n","    \n","    # logits and targets for loss value \n","    loss = tf.nn.softmax_cross_entropy_with_logits( logits=logits,labels = y_reshaped )\n","    loss = tf.reduce_mean(loss)\n","    \n","    return loss \n","    \n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWsXhMRYt7Ln","colab_type":"code","colab":{}},"source":["# build optimizer \n","def build_optimizer(  loss , learning_rate, grad_clip):\n","    tvars = tf.trainable_variables( )\n","    grads, _ = tf.clip_by_global_norm(   tf.gradients(loss, tvars )  , grad_clip )\n","    train_op = tf.train.AdamOptimizer( learning_rate=learning_rate )\n","    optimizer  = train_op.apply_gradients(  zip( grads, tvars ))\n","    \n","    return optimizer "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OabkxcsIt7Lt","colab_type":"code","colab":{}},"source":["# construct CharRNN \n","class CharRNN:\n","    def __init__( self, num_classes, num_seqs=32, num_steps = 50 , \n","                 lstm_size=128 , num_layers = 2, learning_rate = 0.001, \n","                grad_clip = 5) :\n","        tf.reset_default_graph()\n","        \n","        # inputs \n","        self.inputs, self.targets, self.keep_prob = build_inputs( num_seqs, num_steps )\n","        \n","        # LSTM   lstm_size, num_layers, batch_size, keep_prob\n","        cell, self.initial_state = build_lstm( lstm_size, num_layers , num_seqs, self.keep_prob   )\n","        \n","        # one hot encoding \n","        x_one_hot = tf.one_hot( self.inputs, num_classes)\n","        \n","        # run RNN \n","        outputs, state = tf.nn.dynamic_rnn( cell, x_one_hot , initial_state=self.initial_state)\n","        self.final_state = state \n","        \n","        # prediction  lstm_output, in_size , out_size \n","        self.prediction, self.logits = build_output( outputs, lstm_size, num_classes )\n","        \n","        # loss: logits, targets, lstm_size, num_classes\n","        self.loss  = build_loss( self.logits,  self.targets, lstm_size, num_classes)\n","        self.optimizer = build_optimizer(self.loss , learning_rate, grad_clip  )\n","        \n","        \n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUN44-Cnt7Lz","colab_type":"code","colab":{}},"source":["# training model \n","# hyper parameter setup \n","num_seqs =  100\n","num_steps = 100\n","lstm_size = 512\n","num_layers =2\n","learning_rate = 0.001\n","keep_prob = 0.5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1iQU6rCDt7MJ","colab_type":"code","colab":{}},"source":["# create folders \n","!mkdir -p checkpoints/shake\n","!mkdir -p logs/2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WqJHRYkt7L5","colab_type":"code","colab":{}},"source":["# run model \n","epochs = 10 \n","\n","# save variables every n \n","save_every_n = 200\n","\n","model  = CharRNN(  num_classes = len(vocab), num_seqs=num_seqs, num_steps = num_steps , \n","                 lstm_size= lstm_size , num_layers = num_layers, learning_rate = learning_rate )\n","\n","saver = tf.train.Saver( max_to_keep = 100)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bf0RZwfat7MC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":773},"outputId":"8f609e12-7837-4b1f-e2d8-1cb8adc81200","executionInfo":{"status":"ok","timestamp":1559004425250,"user_tz":420,"elapsed":630873,"user":{"displayName":"Steven Wang","photoUrl":"https://lh4.googleusercontent.com/-RC2OL2L-CUs/AAAAAAAAAAI/AAAAAAAAD8I/C-L5xQPIj30/s64/photo.jpg","userId":"14479905213551862384"}}},"source":["\n","with tf.Session( ) as sess:\n","    sess.run( tf.global_variables_initializer( ))\n","    \n","    counter = 0 \n","    for  e in range(epochs):\n","        # train network \n","        new_state = sess.run( model.initial_state)\n","        loss = 0 \n","        for x, y, in get_batches(  encoded, num_seqs, num_steps):\n","            counter = counter + 1\n","            start = time.time()\n","            feed = { model.inputs : x, model.targets: y, model.keep_prob : keep_prob, model.initial_state: new_state}\n","            batch_loss, new_state , _  = sess.run( [ model.loss, model.final_state, model.optimizer ] , feed_dict = feed )\n","            \n","            end  =time.time()\n","        \n","            if counter % 100 == 0:\n","                print( 'epochs: {}/{}'.format( e+ 1, epochs) ,\n","                      'counter: {}'.format( counter) ,\n","                      'loss:'.format(batch_loss), \n","                      'time: {} /batch'.format( end -start ) \n","                     )\n","\n","            if counter % save_every_n == 0:\n","                saver.save( sess, 'checkpoints/shake/i{}_l{}.ckpt' .format( counter, lstm_size ))\n","    \n"],"execution_count":104,"outputs":[{"output_type":"stream","text":["epochs: 1/10 counter: 100 loss: time: 0.12998580932617188 /batch\n","epochs: 1/10 counter: 200 loss: time: 0.1343071460723877 /batch\n","epochs: 1/10 counter: 300 loss: time: 0.1359562873840332 /batch\n","epochs: 1/10 counter: 400 loss: time: 0.1449418067932129 /batch\n","epochs: 2/10 counter: 500 loss: time: 0.13666486740112305 /batch\n","epochs: 2/10 counter: 600 loss: time: 0.1373450756072998 /batch\n","epochs: 2/10 counter: 700 loss: time: 0.13715505599975586 /batch\n","epochs: 2/10 counter: 800 loss: time: 0.13495922088623047 /batch\n","epochs: 2/10 counter: 900 loss: time: 0.13182854652404785 /batch\n","epochs: 3/10 counter: 1000 loss: time: 0.14131760597229004 /batch\n","epochs: 3/10 counter: 1100 loss: time: 0.13471174240112305 /batch\n","epochs: 3/10 counter: 1200 loss: time: 0.13852190971374512 /batch\n","epochs: 3/10 counter: 1300 loss: time: 0.13552355766296387 /batch\n","epochs: 4/10 counter: 1400 loss: time: 0.13737893104553223 /batch\n","epochs: 4/10 counter: 1500 loss: time: 0.1370079517364502 /batch\n","epochs: 4/10 counter: 1600 loss: time: 0.14045190811157227 /batch\n","epochs: 4/10 counter: 1700 loss: time: 0.14531564712524414 /batch\n","epochs: 4/10 counter: 1800 loss: time: 0.13514137268066406 /batch\n","epochs: 5/10 counter: 1900 loss: time: 0.13575410842895508 /batch\n","epochs: 5/10 counter: 2000 loss: time: 0.14080095291137695 /batch\n","epochs: 5/10 counter: 2100 loss: time: 0.13047480583190918 /batch\n","epochs: 5/10 counter: 2200 loss: time: 0.14123821258544922 /batch\n","epochs: 6/10 counter: 2300 loss: time: 0.13530707359313965 /batch\n","epochs: 6/10 counter: 2400 loss: time: 0.13440871238708496 /batch\n","epochs: 6/10 counter: 2500 loss: time: 0.13756608963012695 /batch\n","epochs: 6/10 counter: 2600 loss: time: 0.13484716415405273 /batch\n","epochs: 6/10 counter: 2700 loss: time: 0.13936877250671387 /batch\n","epochs: 7/10 counter: 2800 loss: time: 0.13888287544250488 /batch\n","epochs: 7/10 counter: 2900 loss: time: 0.13233518600463867 /batch\n","epochs: 7/10 counter: 3000 loss: time: 0.13585448265075684 /batch\n","epochs: 7/10 counter: 3100 loss: time: 0.13398075103759766 /batch\n","epochs: 8/10 counter: 3200 loss: time: 0.13440752029418945 /batch\n","epochs: 8/10 counter: 3300 loss: time: 0.14330649375915527 /batch\n","epochs: 8/10 counter: 3400 loss: time: 0.13529348373413086 /batch\n","epochs: 8/10 counter: 3500 loss: time: 0.13269662857055664 /batch\n","epochs: 8/10 counter: 3600 loss: time: 0.14902448654174805 /batch\n","epochs: 9/10 counter: 3700 loss: time: 0.13814616203308105 /batch\n","epochs: 9/10 counter: 3800 loss: time: 0.13560271263122559 /batch\n","epochs: 9/10 counter: 3900 loss: time: 0.12845444679260254 /batch\n","epochs: 9/10 counter: 4000 loss: time: 0.1391003131866455 /batch\n","epochs: 9/10 counter: 4100 loss: time: 0.13161015510559082 /batch\n","epochs: 10/10 counter: 4200 loss: time: 0.1364126205444336 /batch\n","epochs: 10/10 counter: 4300 loss: time: 0.1372241973876953 /batch\n","epochs: 10/10 counter: 4400 loss: time: 0.1334095001220703 /batch\n","epochs: 10/10 counter: 4500 loss: time: 0.1406402587890625 /batch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mL_FTvoMt7MX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":403},"outputId":"0d9426cf-5bc3-419c-aa84-0d133e020777","executionInfo":{"status":"ok","timestamp":1559004425254,"user_tz":420,"elapsed":630855,"user":{"displayName":"Steven Wang","photoUrl":"https://lh4.googleusercontent.com/-RC2OL2L-CUs/AAAAAAAAAAI/AAAAAAAAD8I/C-L5xQPIj30/s64/photo.jpg","userId":"14479905213551862384"}}},"source":["tf.train.get_checkpoint_state('checkpoints/shake')"],"execution_count":105,"outputs":[{"output_type":"execute_result","data":{"text/plain":["model_checkpoint_path: \"checkpoints/shake/i4400_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i200_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i400_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i600_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i800_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i1000_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i1200_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i1400_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i1600_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i1800_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i2000_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i2200_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i2400_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i2600_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i2800_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i3000_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i3200_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i3400_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i3600_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i3800_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i4000_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i4200_l512.ckpt\"\n","all_model_checkpoint_paths: \"checkpoints/shake/i4400_l512.ckpt\""]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"code","metadata":{"id":"rCSIEuWq3T_y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFpBz8LRxpJE","colab_type":"code","colab":{}},"source":["def pick_top_n(preds, vocab_size, top_n=2):\n","\n","    p = np.squeeze(preds)\n","    p[np.argsort(p)[:-top_n]] = 0\n","    p = p / np.sum(p)\n","    c = np.random.choice(vocab_size, 1, p=p)[0]\n","    return c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cThojNORzSmq","colab_type":"code","colab":{}},"source":["def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n","    samples = [c for c in prime]\n","    # 1 to 1 prediction \n","    model = CharRNN(num_classes = len(vocab), num_seqs=1, num_steps = 1 , \n","                 lstm_size= lstm_size , num_layers = num_layers, learning_rate = learning_rate  )\n","    saver = tf.train.Saver()\n","    with tf.Session() as sess:\n","       \n","        saver.restore(sess, checkpoint)\n","        new_state = sess.run(model.initial_state)\n","        for c in prime:\n","            x = np.zeros((1, 1))\n","            \n","            x[0,0] = vocab_to_int[c]\n","            feed = {model.inputs: x,\n","                    model.keep_prob: 1.,\n","                    model.initial_state: new_state}\n","            preds, new_state = sess.run([model.prediction, model.final_state], \n","                                         feed_dict=feed)\n","\n","        c = pick_top_n(preds, len(vocab))\n","        \n","        samples.append(int_to_vocab[c])\n","        \n","        \n","        for i in range(n_samples):\n","            x[0,0] = c\n","            feed = {model.inputs: x,\n","                    model.keep_prob: 1.,\n","                    model.initial_state: new_state}\n","            preds, new_state = sess.run([model.prediction, model.final_state], \n","                                         feed_dict=feed)\n","\n","            c = pick_top_n(preds, len(vocab))\n","            samples.append(int_to_vocab[c])\n","        \n","    return ''.join(samples)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryGlPPT_zTTJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1529},"outputId":"958a3605-f639-4a23-8a24-c019b0fa6990","executionInfo":{"status":"ok","timestamp":1559004840239,"user_tz":420,"elapsed":4807,"user":{"displayName":"Steven Wang","photoUrl":"https://lh4.googleusercontent.com/-RC2OL2L-CUs/AAAAAAAAAAI/AAAAAAAAD8I/C-L5xQPIj30/s64/photo.jpg","userId":"14479905213551862384"}}},"source":["checkpoint = \"checkpoints/shake/i4400_l512.ckpt\"\n","samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"I will not do that\")\n","print(\"\\n The follows are the generated texts:\\n\\n\\n\")\n","print(samp)"],"execution_count":111,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from checkpoints/shake/i4400_l512.ckpt\n","\n"," The follows are the generated texts:\n","\n","\n","\n","I will not do that I would say\n","'The true and trum of times to thee.\n","\n","PRINCESS:\n","What say you that the sun at home?\n","\n","LUCIO:\n","It is the sun that should not think on the state.\n","\n","SIMONIDES:\n","What is your charge?\n","\n","LEONTES:\n","What, art thou not to be a stranger to\n","The suit of the contraction?\n","\n","POINS:\n","Well, I will see the truth into the sea, and so\n","despite the senators of the world that would not\n","be said to this person.\n","\n","SIR HUGH EVANS:\n","Why, there is such a soldier that you should be said\n","to take the secrets of a common.\n","\n","Second Citizen:\n","I think the true send foolish shows and the rest\n","and to the moon and the man that I will not\n","see him as the man that I will not be so are\n","that I will not see you all. I will not be a subject\n","that I am a gentleman that I would say they were\n","dispatch'd to the people.\n","\n","PORTIA:\n","I heard you were as like a common proper.\n","\n","SIR TOBY BELCH:\n","We have sent to the senate. I have seen thee a\n","man to my father to make this present the seal of the world.\n","What say you that?\n","\n","Second Citizen:\n","Well, I will serve you. When I will not be as any man.\n","\n","PROTEUS LEONATUS:\n","There were not so with all my heart.\n","\n","PRINCE HENRY:\n","What, will your lady star of me?\n","\n","LEONTES:\n","What, will the sun to seek this constable,\n","That hath so much as the man shall be made?\n","\n","LUCIO:\n","It was a sorrow fool to seek the senate to the\n","state.\n","\n","PORTIA:\n","I would not be as lost, as to the state\n","The sun they should be so. There is no suit,\n","When thou shalt be the field of the storm,\n","Where I with such a truth that would not be\n","The service of the state of men and there.\n","\n","Secress:\n","We should not but the stocks of the senate.\n","\n","PERICLES:\n","I will, to tell.\n","\n","CASSIUS:\n","Why, then, I prithee,\n","As thou dissess' too, thou shalt be thy father.\n","\n","Second Citizen:\n","Why then it were the sense of the sea, with a\n","burning and true trifles, and the subject should\n","be the moon and the man than all the world, they are a\n","stock and thing that would be the shoutefold that\n","world, that they were not a storment that they\n","shall, the maid with a soldier.\n","\n","PRINCE H\n"],"name":"stdout"}]}]}
